{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e7613-023a-4060-90b6-daae6ef23035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#あるブログをスクレイピングし、ブログ記事本文を用いてクラスタリングをしたうえで、結果について自由に考察するためのコードです。\n",
    "#スクレイピングをするURLを取り除いて実行いるため、このコードはerrorが出ます。\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import MeCab\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from scipy.linalg import get_blas_funcs\n",
    "from scipy.linalg.lapack import get_lapack_funcs\n",
    "from scipy.special import psi  \n",
    "try:\n",
    "    from numpy import triu\n",
    "except ImportError:\n",
    "    from scipy.linalg import triu\n",
    "from numpy import triu\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Download necessary NLTK data\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "#Tech　ブログを全て入手するための1ページ目のウェブサイト\n",
    "#base_url = \"\"\n",
    "\n",
    "\n",
    "def get_all_pages(url):\n",
    "    \"\"\"\n",
    "    ページ番号を取得する関数\n",
    "    Args:\n",
    "    - url (str): 取得するページのURL。\n",
    "    \n",
    "    Returns:\n",
    "    - list: ページ番号のリスト。\n",
    "    \"\"\"\n",
    "    r = requests.get(url)\n",
    "    time.sleep(3) # サーバーに負荷をかけないためのスリープ\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    page_na = soup.find(class_=\"wp-pagenavi\")\n",
    "    \n",
    "    pages = []\n",
    "    if page_na:\n",
    "        # 現在のページ番号を取得\n",
    "        current_page_elem = page_na.find(class_=\"current\")\n",
    "        if current_page_elem:\n",
    "            pages.append(int(current_page_elem.text))\n",
    "        \n",
    "        # すべてのページ番号を取得\n",
    "        page_num_elems = page_na.find_all(class_=\"page\")\n",
    "        for page_elem in page_num_elems:\n",
    "            if page_elem.text.isdigit():\n",
    "                pages.append(int(page_elem.text))\n",
    "        \n",
    "        # 重複を削除し、ページ番号をソート\n",
    "        pages = sorted(set(pages))\n",
    "    else:\n",
    "        print(\"Pagination element not found\")\n",
    "    \n",
    "    return pages\n",
    "\n",
    "# すべてのページを取得\n",
    "pages = get_all_pages(base_url)\n",
    "\n",
    "# 記事のURLを保存するリスト\n",
    "urls = []\n",
    "\n",
    "# ページが見つからなかった場合、最初のページのURLを追加\n",
    "if not pages:\n",
    "    urls.append(base_url)\n",
    "else:\n",
    "    last_page = int(pages[-1])  # 最後のページ番号を取得   \n",
    "    urls.append(base_url) # 最初のページのURLを追加   \n",
    "\n",
    "    # 2ページ目以降のURLを追加\n",
    "    for i in range(2, last_page + 1):\n",
    "        #url = \"{}\".format(i)\n",
    "        urls.append(url)\n",
    "\n",
    "#デバグ用\n",
    "print(\"URLs to be scraped:\")\n",
    "print(urls)\n",
    "\n",
    "\n",
    "def scrape_and_save(urls):\n",
    "    \"\"\"\n",
    "    URLリストから必要な情報をスクレイピングする関数\n",
    "\n",
    "    Args:\n",
    "    - urls (list): スクレイピング対象のURLリスト\n",
    "    - output_file (str): CSVファイルの出力先パス\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # 正規表現パターンを定義\n",
    "    #pattern = re.compile(r\"*\")\n",
    "\n",
    "    # URLデータとブログタイトルのデータを保存するリストを初期化\n",
    "    url_list = []\n",
    "    title_list = []\n",
    "\n",
    "    # 各ページから必要な情報をスクレイピング\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        time.sleep(3)  # リクエスト間に3秒の待機時間を挿入\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # href属性を持つすべての<a>タグを探す\n",
    "        links = soup.find_all(\"a\", href=True)\n",
    "\n",
    "        # URLとタイトルを抽出\n",
    "        for link in links:\n",
    "            href = link[\"href\"]\n",
    "            # URLがtechブログのパターンに一致するかチェック\n",
    "            if pattern.match(href):\n",
    "                # タイトルを抽出\n",
    "                title_elem = link.find(\"p\", class_=\"c-news-title\")\n",
    "                if title_elem is not None:\n",
    "                    title = title_elem.text.strip()\n",
    "                    url_list.append(href)\n",
    "                    title_list.append(title)\n",
    "\n",
    "    # 収集したデータからDataFrameを作成\n",
    "    df_links = pd.DataFrame({\"Title\": title_list, \"URL\": url_list})\n",
    "\n",
    "    return df_links,title_list, url_list\n",
    "\n",
    "\n",
    "# スクレイピングしてDataFrameを取得\n",
    "df_links,title_list, url_list = scrape_and_save(urls)\n",
    "print(df_links)\n",
    "# 処理するURLのリスト\n",
    "urls = url_list\n",
    "\n",
    "# 取り出したい品詞\n",
    "select_conditions = ['名詞']\n",
    "\n",
    "#MeCabのタグオブジェクトを生成\n",
    "tagger = MeCab.Tagger('')\n",
    "\n",
    "# MeCabのparseメソッドを一度実行して安定させる\n",
    "tagger.parse('')\n",
    "\n",
    "def wakati_text(text):\n",
    "    \"\"\"\n",
    "    日本語の名詞と形容詞を抽出する関数\n",
    "    Args:\n",
    "    - text (str): 分析するテキスト\n",
    "    \n",
    "    Returns:\n",
    "    - list: 抽出された単語のリスト\n",
    "    \"\"\"\n",
    "    tagger = MeCab.Tagger('')  # MeCabのタグオブジェクトを生成\n",
    "    tagger.parse('')  # 初期化のためにparseメソッドを一度実行\n",
    "    node = tagger.parseToNode(text)  # テキストを解析してノードのリストを取得\n",
    "    terms = []\n",
    "    while node:\n",
    "        term = node.surface  # ノードの表層形を取得\n",
    "        pos = node.feature.split(',')[0]  # ノードの品詞情報を取得\n",
    "        if pos in select_conditions:  # 品詞が指定した条件に一致する場合\n",
    "            terms.append(term)  # リストに単語を追加\n",
    "        node = node.next  # 次のノードに移動\n",
    "    return terms\n",
    "\n",
    "def extract_nouns(text):\n",
    "    \"\"\"\n",
    "    英語の名詞を抽出する関数\n",
    "    Args:\n",
    "    - text (str): 分析するテキスト\n",
    "    \n",
    "    Returns:\n",
    "    - list: 抽出された名詞のリスト\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)  # テキストを単語に分割\n",
    "    pos_tags = nltk.pos_tag(words)  # 単語に品詞タグを付与\n",
    "    nouns = [word for word, pos in pos_tags if pos in ['NN', 'NNS', 'NNP', 'NNPS']]  # 名詞に該当する単語を抽出\n",
    "    return nouns\n",
    "\n",
    "\n",
    "def stopwords(words, lang):\n",
    "    \"\"\"\n",
    "    指定された言語に基づいて、リスト内の不要な単語をフィルタリングする関数\n",
    "    Args:\n",
    "    - words (list): フィルタリングする単語のリスト。\n",
    "    - lang (str): 言語コード（'jp' は日本語、'en' は英語）。\n",
    "\n",
    "    Returns:\n",
    "    - list: フィルタリングされた単語のリスト。\n",
    "    \"\"\"\n",
    "    # 正規表現パターン：不要な単語をフィルタリングするためのパターン\n",
    "    numb = r\"[0-9０-９]+$\"           # 数字のみのパターン\n",
    "    english = r\"^[a-zA-Z]$\"          # 英字のみのパターン\n",
    "    pair = r\"^[ぁ-んァ-ン]{2}$\"      # 2文字のみのパターン\n",
    "    pair_english = r\"^[a-zA-Z]{2}$\"\n",
    "    single = r\"^[0-9０−９ぁ-んァ-ン一-龥!?%!-/=:@{~\\u3001-\\u303F]$\"  # 特定の文字のみのパターン\n",
    "    if lang == 'jp':\n",
    "        return [x for x in words if re.match(pair, x) is None and re.match(pair_english, x) is None and re.match(single, x) is None and re.match(numb, x) is None]\n",
    "    elif lang == 'en':\n",
    "        return [x for x in words if re.match(pair, x) is None and re.match(single, x) is None and re.match(numb, x) is None and re.match(pair_english, x) is None and  re.match(english, x) is None]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "# データフレームを格納するリスト\n",
    "dfs = []\n",
    "\n",
    "# 各URLに対して処理を行う\n",
    "for i, url in enumerate(urls, 1):\n",
    "    # Chromeドライバーを初期化\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    browser = webdriver.Chrome(service=service)\n",
    "    \n",
    "    browser.get(url)  # 指定されたURLをブラウザで開く\n",
    "    time.sleep(3)  # ページの読み込みを待つ\n",
    "\n",
    "    # ページソースを取得してBeautifulSoupで解析\n",
    "    page_source = browser.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # ページのテキストを取得\n",
    "    text = soup.get_text()\n",
    "\n",
    "    \n",
    "    japanese_terms = wakati_text(text)  # 日本語の形容詞と名詞を抽出\n",
    "    japanese_pattern = r'^[ぁ-んァ-ン一-龥]+$'#大文字英語　Allなどが出てきたためこのフィルターで英単語排除\n",
    "    filtered_japanese_terms = [term for term in japanese_terms if re.match(japanese_pattern, term) is not None]\n",
    "    filtered_japanese_terms = stopwords(filtered_japanese_terms, 'jp')\n",
    "\n",
    "    # 英語の名詞を含む文を正規表現で抽出するパターン\n",
    "    english_pattern = r'(?<![^\\W\\d_])[A-Za-z]+(?![^\\W\\d_])'\n",
    "\n",
    "    \n",
    "    english_term = re.findall(english_pattern, text)  # 英語の単語を正規表現で抽出\n",
    "\n",
    "    # 英語の単語を小文字に変換　- 大文字と小文字での単語を統一するため（ex: Slackと slack）\n",
    "    english_terms_lower = [x.lower() for x in english_term]\n",
    "\n",
    "    # 英語の単語からストップワードを除外\n",
    "    english_terms = stopwords(english_terms_lower, 'en')\n",
    "\n",
    "    # 英語の単語から名詞を抽出\n",
    "    pos_tags = nltk.pos_tag(english_terms_lower)\n",
    "    english_noun = [word for word, pos in pos_tags if pos.startswith('NN')]\n",
    "    english_terms = stopwords(english_noun, 'en')\n",
    "    \n",
    "\n",
    "    # 日本語単語が入っているリストと英語の単語が入っているリストを結合\n",
    "    combined_terms = filtered_japanese_terms + english_terms\n",
    "    \n",
    "    # 結合された単語をストップワード関数でフィルタリング\n",
    "    filtered_terms = stopwords(combined_terms, 'jp')\n",
    "    \n",
    "    # 単語の頻度を計算\n",
    "    frequency = Counter(filtered_terms)\n",
    "    \n",
    "    # 最も頻度の高い上位500語を取得\n",
    "    top_30_terms = frequency.most_common(500)\n",
    "\n",
    "    # データフレームを \"data_number\"(各ウェブサイトを管理しやすくするためナンバリング) 列とともに作成する\n",
    "    df_terms_with_data_number = pd.DataFrame({\n",
    "        \"Term\": [term for term, _ in top_30_terms],\n",
    "        \"Frequency\": [freq for _, freq in top_30_terms],\n",
    "        \"data_number\": [i] * len(top_30_terms)})\n",
    "\n",
    "    # データフレームをリストに追加する\n",
    "    dfs.append(df_terms_with_data_number)\n",
    "    # CSV として保存する\n",
    "    file_name = \"/Users/itagakikouki//dependabot_top_500_terms.csv\"\n",
    "    df_terms_with_data_number.to_csv(file_name, index=False)\n",
    "    \n",
    "    # 結果をファイルに保存する\n",
    "    with open(\"dependabot_top_30_terms.txt\", \"w\") as f:\n",
    "        for term, freq in top_30_terms:\n",
    "            f.write(f\"{term}: {freq}\\n\")\n",
    "\n",
    "# 全てのデータフレームを1つに結合する\n",
    "result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# CSV として保存する\n",
    "result_df.to_csv(\"output_terms.csv\", index=False)\n",
    "\n",
    "# タイトルとURLからデータフレームを作成する\n",
    "df_links = pd.DataFrame({\"Title\": title_list, \"URL\": url_list, \"data_number\": range(1, len(title_list) + 1)})\n",
    "\n",
    "# data_number をキーにして dfs と df_links をマージする\n",
    "result_df_with_links = result_df.merge(df_links, on=\"data_number\", how=\"left\")\n",
    "\n",
    "# マージしたデータフレームを CSV ファイルに保存する\n",
    "file_name = \"/Users/itagakikouki//dependabot_top_30_terms_with_links.csv\"\n",
    "result_df_with_links.to_csv(file_name, index=False)\n",
    "\n",
    "# CSV として保存する\n",
    "result_df.to_csv(\"/Users/itagakikouki//dependabot_top_500_terms.csv\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9779d-d94e-4608-aa93-5d4033cc417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 全ての文書をリストに変換\n",
    "documents = []\n",
    "data_numbers = []  # data_numberを保存するリスト\n",
    "for i, df in enumerate(dfs):\n",
    "    terms = df['Term'].tolist()\n",
    "    documents.append(terms)\n",
    "    data_numbers.append(i+1)  # data_numberを追加\n",
    "\n",
    "# Word2Vecモデルの訓練\n",
    "word2vec_model = Word2Vec(sentences=documents, vector_size=1000, window=5, min_count=1, workers=4)\n",
    "\n",
    "def document_vector(model, doc):\n",
    "    \"\"\"\n",
    "    文書の特徴ベクトルを生成する関数\n",
    "\n",
    "    Args:\n",
    "    - model: Word2Vecモデル\n",
    "    - doc (list): 文書を構成する単語のリスト\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: 文書の特徴ベクトル\n",
    "    \"\"\"\n",
    "    # 各単語のベクトルを取得\n",
    "    word_vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "    # 単語ベクトルの平均を特徴ベクトルとする\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "feature_vectors = []\n",
    "for i in range(len(documents)):\n",
    "    doc = documents[i]\n",
    "    feature_vectors.append(document_vector(word2vec_model, doc))\n",
    "\n",
    "# 特徴ベクトルをDataFrameに変換\n",
    "feature_df = pd.DataFrame(feature_vectors)\n",
    "feature_df['data_number'] = data_numbers  # data_number列を追加\n",
    "\n",
    "\n",
    "\n",
    "def plot_elbow_method(feature_df, max_clusters=10):\n",
    "    \"\"\"\n",
    "    エルボーカーブを用いてクラスタ数を決定し、グラフを描画する関数\n",
    "\n",
    "    Args:\n",
    "    - feature_df (DataFrame): クラスタリング対象の特徴量が含まれたDataFrame\n",
    "    - max_clusters (int): 最大クラスタ数（デフォルトは10）\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    distortions = []\n",
    "    for i in range(1, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=i, random_state=0)\n",
    "        kmeans.fit(feature_df.drop('data_number', axis=1))\n",
    "        distortions.append(kmeans.inertia_)\n",
    "\n",
    "    # エルボーカーブをプロット\n",
    "    plt.plot(range(1, max_clusters + 1), distortions, marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plot_elbow_method(feature_df)\n",
    "# 最適なクラスタ数を選択\n",
    "optimal_k = 4 \n",
    "\n",
    "# クラスタリングの実行\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=0)\n",
    "kmeans.fit(feature_df.drop('data_number', axis=1))\n",
    "\n",
    "# クラスタリング結果をDataFrameに追加\n",
    "feature_df['cluster'] = kmeans.labels_\n",
    "\n",
    "# 結果をCSVに保存\n",
    "feature_df.to_csv(\"clustered_word2vec_feature_matrix.csv\", index=False)\n",
    "\n",
    "\n",
    "def plot_pca_with_clustering(feature_df, optimal_k):\n",
    "    \"\"\"\n",
    "    PCAで特徴ベクトルを2次元に圧縮し、クラスタリング結果をプロットする関数\n",
    "    \n",
    "    Parameters:\n",
    "    - feature_df: DataFrame, 特徴ベクトルのデータフレーム。\n",
    "    - optimal_k: int, K-meansクラスタの数。\n",
    "    \"\"\"\n",
    "    # PCAで次元削減\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(feature_df.drop(['data_number', 'cluster'], axis=1))\n",
    "    \n",
    "    # プロットの準備\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "    markers = ['o', 's', 'D', '^', 'v', '>', '<']\n",
    "    \n",
    "    # クラスタリング結果をプロット\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for cluster_num in range(optimal_k):\n",
    "        cluster_data = feature_df[feature_df['cluster'] == cluster_num]\n",
    "        plt.scatter(pca_result[cluster_data.index, 0], pca_result[cluster_data.index, 1],\n",
    "                    color=colors[cluster_num], marker=markers[cluster_num], label=f'Cluster {cluster_num}')\n",
    "    \n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('PCA of Word2Vec Feature Vectors with Clustering')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_pca_with_clustering(feature_df, optimal_k)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e00959-9879-4be9-bc16-f84dd2f16c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature_df  = feature_df[['data_number', 'cluster']]\n",
    "\n",
    "# data_numberをキーにして結合\n",
    "merged_df = new_feature_df.merge(df_links, on='data_number',how=\"left\")\n",
    "\n",
    "# CSVファイルを読み込む\n",
    "data = pd.read_csv(\"/Users/itagakikouki//Google_Analytics_Page_Views.csv\")\n",
    "\n",
    "\n",
    "# 必要な列を選択する\n",
    "selected_data = data[['event_params_page_title']]\n",
    "#統合させるため列の名前の統一化\n",
    "selected_data = data.rename(columns={'event_params_page_title': 'Title'})\n",
    "\n",
    "\n",
    "\n",
    "def process_and_merge_data(selected_data, merged_df, file_name):\n",
    "    \"\"\"\n",
    "    'Title' 列の「|」の前の部分を抽出して新しい列 'Title_prefix' を作成し、記事ごとのページビューを計算して結合する関数。\n",
    "    \n",
    "    Parameters:\n",
    "    - selected_data: DataFrame, 処理対象のデータフレーム。\n",
    "    - merged_df: DataFrame, 既存のデータフレーム。\n",
    "    - file_name: str, 保存先のファイル名（CSV）。\n",
    "    \n",
    "    Returns:\n",
    "    - mergedd_df: DataFrame, 結合後のデータフレーム。\n",
    "    \"\"\"\n",
    "    # 'Title' 列の「|」の前の部分を抽出して新しい列 'Title_prefix' を作成\n",
    "    selected_data['Title_prefix'] = selected_data['Title'].str.split('|').str[0].str.lower().str.strip()\n",
    "\n",
    "    # 記事ごとのページビューを計算する\n",
    "    page_views = selected_data.groupby('Title_prefix').size().reset_index(name='page_views')\n",
    "    page_views = page_views.sort_values(by='page_views', ascending=False)\n",
    "\n",
    "    # 既存のデータフレーム 'merged_df' も同様に 'Title' 列の「|」の前の部分を抽出して 'Title_prefix' 列を作成\n",
    "    merged_df['Title_prefix'] = merged_df['Title'].str.split('|').str[0].str.lower().str.strip()\n",
    "\n",
    "    # 'Title_prefix' をキーにして結合\n",
    "    mergedd_df = merged_df.merge(page_views, on=\"Title_prefix\", how=\"inner\")\n",
    "    print(mergedd_df)\n",
    "\n",
    "    # CSVファイルに保存\n",
    "    mergedd_df.to_csv(file_name, index=False)\n",
    "\n",
    "    return mergedd_df\n",
    "\n",
    "# selected_data と merged_df が既に定義されている場合\n",
    "file_name = \"/Users/itagakikouki//page_views.csv\"\n",
    "mergedd_df = process_and_merge_data(selected_data, merged_df, file_name)\n",
    "\n",
    "\n",
    "'''\n",
    "データセットのevent_dateによるとpage_viewが行われた最新日が2023年9月30日\n",
    "data_numberが０から４のデータはそれ以降に新しく投稿されたブログな為、event_params_page_titleに表示されていない\n",
    "0                   Slack botでOpsgenieのオンコール担当者をメンションする   \n",
    "1                         Dependabot アラート による脆弱性対策の分析   0\n",
    "2               フライウィール初の OSS “johari-mirror” を公開しました   1\n",
    "3                    フライウィールにおける protovalidate の導入と活用   1\n",
    "4    AWS Lambda で Amazon Aurora のスロークエリログを Slack 通知する   0\n",
    "'''\n",
    "\n",
    "# クラスターごとの総ページビューを計算\n",
    "cluster_page_views = mergedd_df.groupby('cluster')['page_views'].sum().reset_index()\n",
    "total_page_views = cluster_page_views['page_views'].sum()\n",
    "\n",
    "# クラスターごとのページビューの割合を計算\n",
    "cluster_page_views['percentage'] = (cluster_page_views['page_views'] / total_page_views) * 100\n",
    "\n",
    "\n",
    "\n",
    "def plot_cluster_page_views(cluster_page_views):\n",
    "    \"\"\"\n",
    "    クラスタごとのページビューの割合を棒グラフで表示する関数\n",
    "\n",
    "    Args:\n",
    "    - cluster_page_views (DataFrame): クラスタごとのページビュー割合が含まれるDataFrame\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # 棒グラフを作成\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(cluster_page_views['cluster'], cluster_page_views['percentage'], color='skyblue')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Percentage of Page Views')\n",
    "    plt.title('Percentage of Page Views by Cluster')\n",
    "    plt.xticks(cluster_page_views['cluster'], ['Cluster {}'.format(int(c)) for c in cluster_page_views['cluster']])\n",
    "    plt.ylim(0, 100)\n",
    "\n",
    "    # 割合をラベルとして表示\n",
    "    for i, value in enumerate(cluster_page_views['percentage']):\n",
    "        plt.text(i, value + 1, f'{value:.1f}%', ha='center', va='bottom')\n",
    "    # グラフを表示\n",
    "    plt.show()\n",
    "\n",
    "def plot_cluster_page_views_pie(merged_df):\n",
    "    \"\"\"\n",
    "    クラスタごとのページビューの割合を円グラフで表示する関数\n",
    "\n",
    "    Args:\n",
    "    - merged_df (DataFrame): クラスタごとのページビュー割合が含まれるDataFrame\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # クラスターごとのページビューの割合を計算\n",
    "    clusters = merged_df['cluster'].unique()\n",
    "    num_clusters = len(clusters)\n",
    "\n",
    "     # クラスタの順序に合わせてソート\n",
    "    clusters.sort()\n",
    "\n",
    "    # クラスターごとに円グラフを作成\n",
    "    fig, axs = plt.subplots(1, num_clusters, figsize=(15, 5))\n",
    "\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        cluster_data = merged_df[merged_df['cluster'] == cluster]\n",
    "        page_view_counts = cluster_data['page_views']\n",
    "\n",
    "        axs[i].pie(page_view_counts,autopct='%1.1f%%', startangle=90, counterclock=False)\n",
    "        axs[i].set_title(f'Cluster {int(cluster)}')\n",
    "        axs[i].axis('equal')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_stacked_bar(merged_df):\n",
    "    \"\"\"\n",
    "    クラスターごとのデータをピボットテーブルに変換し、積み上げ棒グラフを描画する関数\n",
    "\n",
    "    Args:\n",
    "    - merged_df (DataFrame): クラスターごとのデータが含まれたDataFrame\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # クラスターごとのデータをピボットテーブルに変換\n",
    "    pivot_df = merged_df.pivot_table(index='cluster', columns='Title_prefix', values='page_views', aggfunc='sum', fill_value=0)\n",
    "\n",
    "    # クラスターごとの総ページビューを計算\n",
    "    cluster_totals = pivot_df.sum(axis=1)\n",
    "\n",
    "    # グラフを描画\n",
    "    pivot_df.plot(kind='bar', stacked=True, figsize=(15, 8), colormap='tab20')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Page Views')\n",
    "    plt.title('Page Views by Cluster and Title')\n",
    "    plt.legend(title='Title Prefix', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # フォントを日本語対応に設定\n",
    "    plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "    plt.rcParams['font.sans-serif'] = ['Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meiryo', 'MS Gothic', 'sans-serif']\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e644d4a-f36c-4665-965a-b12d587ffc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#外れ値を予測するためグラフの表示\n",
    "\n",
    "#閲覧数を高い順に表示\n",
    "sorted_df = mergedd_df.sort_values(by='page_views',ascending=False)[[\"Title\",\"page_views\"]]\n",
    "print(sorted_df)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_page_views_histogram(df, column_name, bins=10, edgecolor='black'):\n",
    "    \"\"\"\n",
    "    データフレーム内の指定された列のヒストグラムをプロットする関数\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame, データフレーム。\n",
    "    - column_name: str, ヒストグラムを作成する列の名前。\n",
    "    - bins: int, ヒストグラムのビンの数（デフォルトは10）。\n",
    "    - edgecolor: str, ビンのエッジの色（デフォルトは 'black'）。\n",
    "    \"\"\"\n",
    "    plt.hist(df[column_name], bins=bins, edgecolor=edgecolor)\n",
    "    plt.xlabel('Page Views')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Page Views')\n",
    "    plt.show()\n",
    "\n",
    "plot_page_views_histogram(mergedd_df, \"page_views\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_page_views_boxplot(df, column_name, figsize=(8, 6), vert=True):\n",
    "    \"\"\"\n",
    "    データフレーム内の指定された列の箱ひげ図をプロットする関数\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame, データフレーム。\n",
    "    - column_name: str, 箱ひげ図を作成する列の名前。\n",
    "    - figsize: tuple, 図のサイズ（デフォルトは (8, 6)）。\n",
    "    - vert: bool, 箱ひげ図の方向（デフォルトは True: 垂直）。\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)  # 図のサイズを設定\n",
    "    plt.boxplot(df[column_name], vert=vert)  # vert=False で水平方向に描画\n",
    "    plt.xlabel('Page Views')\n",
    "    plt.title('Boxplot of Page Views')\n",
    "    plt.show()\n",
    "\n",
    "plot_page_views_boxplot(mergedd_df, \"page_views\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_pca_with_clustering(feature_df, optimal_k):\n",
    "    \"\"\"\n",
    "    PCAで特徴ベクトルを2次元に圧縮し、クラスタリング結果をプロットする関数\n",
    "    \n",
    "    Parameters:\n",
    "    - feature_df: DataFrame, 特徴ベクトルのデータフレーム。\n",
    "    - optimal_k: int, K-meansクラスタの数。\n",
    "    \"\"\"\n",
    "    # PCAで次元削減\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(feature_df.drop(['data_number', 'cluster'], axis=1))\n",
    "    print(pca_result)\n",
    "    \n",
    "    # プロットの準備\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "    markers = ['o', 's', 'D', '^', 'v', '>', '<']\n",
    "    \n",
    "    # クラスタリング結果をプロット\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for cluster_num in range(optimal_k):\n",
    "        cluster_data = feature_df[feature_df['cluster'] == cluster_num]\n",
    "        plt.scatter(pca_result[cluster_data.index, 0], pca_result[cluster_data.index, 1],\n",
    "                    color=colors[cluster_num], marker=markers[cluster_num], label=f'Cluster {cluster_num}')\n",
    "    \n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('PCA of Word2Vec Feature Vectors with Clustering')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_pca_with_clustering(feature_df, optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206f4e6b-37f1-46f7-bdb4-ec1288030090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#外れ値をより詳しく分析\n",
    "\n",
    "#cluster　別のタイトル取得\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#クラスター別のブログタイトル、閲覧数の表示\n",
    "cluster_mergedd_df = mergedd_df.sort_values(by='cluster', ascending=True)\n",
    "print(cluster_mergedd_df[['cluster', 'page_views','Title']])\n",
    "#外れ値の削除\n",
    "mergedd_df = mergedd_df[mergedd_df['Title'] != 'dbt とは何をするツールなのか？']\n",
    "# 各クラスターごとに特徴量の平均値を計算\n",
    "cluster_means = mergedd_df.groupby('cluster')['page_views'].mean()\n",
    "print(\"クラスターごとの平均値：\",cluster_means)\n",
    "# 各クラスターごとに特徴量の中央値を計算\n",
    "cluster_medians = mergedd_df.groupby('cluster')['page_views'].median()\n",
    "print(\"クラスターごとの中央値：\",cluster_medians)\n",
    "\n",
    "\n",
    "\n",
    "cluster_mergedd_df = mergedd_df.groupby('cluster')['page_views'].sum().reset_index()\n",
    "total_page_views = cluster_page_views['page_views'].sum()\n",
    "cluster_mergedd_df['percentage'] = (cluster_mergedd_df['page_views'] / total_page_views) * 100\n",
    "\n",
    "\n",
    "plot_cluster_page_views(cluster_mergedd_df) #棒グラフ\n",
    "#pie chart\n",
    "plot_cluster_page_views_pie(mergedd_df)\n",
    "\n",
    "merged_df_sorted = mergedd_df.sort_values(by='page_views', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# page_views 列を取得\n",
    "page_views = mergedd_df['page_views']\n",
    "\n",
    "\n",
    "# 第一四分位数（Q1）、第三四分位数（Q3）を計算\n",
    "Q1 = page_views.quantile(0.25)\n",
    "Q3 = page_views.quantile(0.75)\n",
    "\n",
    "# IQRを計算\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 外れ値の基準を計算\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "print(\"Q1:\", Q1)\n",
    "print(\"Q3:\", Q3)\n",
    "print(\"IQR:\", IQR)\n",
    "print(\"Lower Bound:\", lower_bound)\n",
    "print(\"Upper Bound:\", upper_bound)\n",
    "\n",
    "\n",
    "# 外れ値を抽出\n",
    "outliers = mergedd_df[(page_views < lower_bound) | (page_views > upper_bound)]\n",
    "print(\"This is outliers: \",outliers)\n",
    "\n",
    "\n",
    "# 外れ値を削除\n",
    "filtered_df = mergedd_df[(page_views >= lower_bound) & (page_views <= upper_bound)]\n",
    "\n",
    "#外れ値を全て削除した場合の棒グラフの作成\n",
    "cluster_filtered_df = filtered_df.groupby('cluster')['page_views'].sum().reset_index()\n",
    "total_page_views = cluster_page_views['page_views'].sum()\n",
    "cluster_filtered_df['percentage'] = (cluster_filtered_df['page_views'] / total_page_views) * 100\n",
    "plot_cluster_page_views(cluster_filtered_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8cdce-2cc9-40ff-a875-153b9a606c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#クラスターごとに重要な単語(頻出単語)の抽出\n",
    "\n",
    "\n",
    "\n",
    "#display_top_terms_per_cluster関数を用いるためにクラスターとTerms,閲覧数があるデータフレームの作成\n",
    "dfs_combined = pd.concat(dfs, ignore_index=True)\n",
    "count_frequent_words = dfs_combined.merge(new_feature_df, on=\"data_number\", how=\"left\")\n",
    "\n",
    "\n",
    "def display_top_terms_per_cluster(data, top_n=20):\n",
    "    \"\"\"\n",
    "    クラスターごとに同じ用語の頻度を合算し、各用語の合計頻度を取得し、上位の頻度を表示する関数\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame, 用語と頻度のデータフレーム。列名は 'cluster', 'Term', 'Frequency'。\n",
    "    - top_n: int, クラスターごとに表示する上位の用語数（デフォルトは20）。\n",
    "    \"\"\"\n",
    "    # クラスターごとに同じ用語の頻度を合算する\n",
    "    summed_freq_per_cluster = data.groupby(['cluster', 'Term'])['Frequency'].sum().reset_index()\n",
    "\n",
    "    # クラスターごとに上位の頻度を表示する\n",
    "    for cluster in summed_freq_per_cluster['cluster'].unique():\n",
    "        print(f\"Cluster {cluster}:\")\n",
    "        top_terms = summed_freq_per_cluster[summed_freq_per_cluster['cluster'] == cluster].nlargest(top_n, 'Frequency')\n",
    "        print(top_terms[['Term', 'Frequency']])\n",
    "        print()\n",
    "\n",
    "\n",
    "display_top_terms_per_cluster(count_frequent_words, top_n=20)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c510924-8649-4f73-9bd0-e5643f508b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e802f3a-c5d4-44d2-a549-e62cffed78cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbcb40c-5c28-4566-9d90-f5a52d0b0ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
